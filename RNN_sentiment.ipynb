{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import requests\n",
    "requests.packages.urllib3.disable_warnings()\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    # Legacy Python that doesn't verify HTTPS certificates by default\n",
    "    pass\n",
    "else:\n",
    "    # Handle target environment that doesn't support HTTPS verification\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# number of words to be considered, selects based on frequency\n",
    "# too big of a vocab size can lead to overfitting\n",
    "vocab_size = 10000\n",
    "\n",
    "# max number of words in each sequence\n",
    "max_length = 250"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# load the IMDB dataset\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# pads the shorter sequences with zeroes, and truncates the sequences that are too long\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_length)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# define the RNN\n",
    "# RNNs are for sequences of data, CNNs are for grids of data \n",
    "model = Sequential([\n",
    "    \n",
    "    # translates words to number vectors\n",
    "    # output_dim is number of features\n",
    "    Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length),\n",
    "\n",
    "    # captures relationships between neurons\n",
    "    SimpleRNN(units=64, return_sequences=True),\n",
    "    \n",
    "    SimpleRNN(units=32),\n",
    "    \n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# compile\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# train\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=5, validation_split=0.2)\n",
    "\n",
    "# evaluate\n",
    "loss, accuracy = model.evaluate(x_test, y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 47s 73ms/step - loss: 0.6288 - accuracy: 0.6385 - val_loss: 0.4906 - val_accuracy: 0.7760\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 45s 72ms/step - loss: 0.4594 - accuracy: 0.7904 - val_loss: 0.4953 - val_accuracy: 0.7634\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 45s 72ms/step - loss: 0.3282 - accuracy: 0.8652 - val_loss: 0.4020 - val_accuracy: 0.8376\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 45s 71ms/step - loss: 0.5208 - accuracy: 0.7406 - val_loss: 0.5654 - val_accuracy: 0.7240\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 45s 72ms/step - loss: 0.4008 - accuracy: 0.8182 - val_loss: 0.6242 - val_accuracy: 0.6906\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.6181 - accuracy: 0.6904\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "prediction = model.predict(x_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "782/782 [==============================] - 13s 16ms/step\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "prediction"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.0890338 ],\n",
       "       [0.9636415 ],\n",
       "       [0.3486933 ],\n",
       "       ...,\n",
       "       [0.13828507],\n",
       "       [0.25446862],\n",
       "       [0.90883505]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "index = imdb.get_word_index()\n",
    "reverse_index = dict([(value, key) for (key, value) in index.items()]) \n",
    "decoded = \" \".join( [reverse_index.get(i - 3, \"#\") for i in x_test[1000]] )\n",
    "print(decoded) "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # this is one of the worst movies i have ever seen the # for the film is better than the film itself my girlfriend and i watched it this past weekend and we only continued to watch it in the hopes that it would get better it didn't br br the picture quality is poor it looks like it was shot on video and transferred to film the lighting is not great which makes it harder to read the actors' facial expressions the acting itself was cheesy but i guess it's acceptable for yet another teenage horror flick the sound was a huge problem sometimes you have to # the video because the sound is unclear and or # br br it holds no real merit of it's own trying to ride on the # of sleepy hollow don't bother with this one\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "prediction[1000]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.07530642], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.10.1",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.10.1 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}